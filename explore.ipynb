{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import psycopg2\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')\n",
    "\n",
    "DWH_CLUSTER_TYPE       = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES          = config.get(\"DWH\",\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE          = config.get(\"DWH\",\"DWH_NODE_TYPE\")\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "\n",
    "DB_NAME                 = config.get(\"CLUSTER\",\"DB_NAME\")\n",
    "DB_USER            = config.get(\"CLUSTER\",\"DB_USER\")\n",
    "DB_PASSWORD        = config.get(\"CLUSTER\",\"DB_PASSWORD\")\n",
    "DB_PORT               = config.get(\"CLUSTER\",\"DB_PORT\")\n",
    "\n",
    "DWH_IAM_ROLE_NAME      = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")\n",
    "\n",
    "SONG_DATA = config.get('S3','SONG_DATA')\n",
    "LOG_DATA = config.get('S3','LOG_DATA')\n",
    "BUCKET = config.get('S3','BUCKET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(BUCKET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2 = boto3.resource('ec2', region_name='us-west-2', aws_access_key_id=KEY, aws_secret_access_key=SECRET)\n",
    "s3 = boto3.resource('s3', region_name='us-west-2', aws_access_key_id=KEY, aws_secret_access_key=SECRET)\n",
    "iam = boto3.client('iam', region_name='us-west-2', aws_access_key_id=KEY, aws_secret_access_key=SECRET)\n",
    "redshift = boto3.client('redshift', region_name='us-west-2', aws_access_key_id=KEY, aws_secret_access_key=SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.ObjectSummary(bucket_name='udacity-dend', key='log_data/2018/11/2018-11-01-events.json')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15 entries, 0 to 14\n",
      "Data columns (total 18 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   artist         11 non-null     object \n",
      " 1   auth           15 non-null     object \n",
      " 2   firstName      15 non-null     object \n",
      " 3   gender         15 non-null     object \n",
      " 4   itemInSession  15 non-null     int64  \n",
      " 5   lastName       15 non-null     object \n",
      " 6   length         11 non-null     float64\n",
      " 7   level          15 non-null     object \n",
      " 8   location       15 non-null     object \n",
      " 9   method         15 non-null     object \n",
      " 10  page           15 non-null     object \n",
      " 11  registration   15 non-null     int64  \n",
      " 12  sessionId      15 non-null     int64  \n",
      " 13  song           11 non-null     object \n",
      " 14  status         15 non-null     int64  \n",
      " 15  ts             15 non-null     int64  \n",
      " 16  userAgent      15 non-null     object \n",
      " 17  userId         15 non-null     int64  \n",
      "dtypes: float64(1), int64(6), object(11)\n",
      "memory usage: 2.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "log_data_bucket = s3.Bucket(BUCKET)\n",
    "\n",
    "# json_df = None\n",
    "# count = 0\n",
    "# for my_object in log_data_bucket.objects.filter(Prefix='log_data'):\n",
    "#     s3_client = boto3.client('s3', region_name='us-west-2', aws_access_key_id=KEY, aws_secret_access_key=SECRET)\n",
    "#     json_obj = s3_client.get_object(Bucket=BUCKET, Key=my_object.key)\n",
    "#     json_data = json_obj[\"Body\"].read().decode('utf-8')\n",
    "#     if not json_data:\n",
    "#         print(\"Skipping empty\", my_object.key)\n",
    "#         continue\n",
    "#     else:\n",
    "        \n",
    "#         count += 1\n",
    "#         if count == 1:\n",
    "#             json_df = pd.read_json(json_data, lines=True)\n",
    "        \n",
    "# print(json_df.info())\n",
    "\n",
    "first_object = None\n",
    "\n",
    "for my_object in log_data_bucket.objects.filter(Prefix='log_data').limit(2):\n",
    "    if my_object.key.endswith('.json'):\n",
    "        first_object = my_object\n",
    "        print(my_object)\n",
    "\n",
    "s3_client = boto3.client('s3', region_name='us-west-2', aws_access_key_id=KEY, aws_secret_access_key=SECRET)\n",
    "json_obj = s3_client.get_object(Bucket=BUCKET, Key=first_object.key)\n",
    "json_data = json_obj[\"Body\"].read().decode('utf-8')\n",
    "json_df = pd.read_json(json_data, lines=True)\n",
    "print(json_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.ObjectSummary(bucket_name='udacity-dend', key='song-data/A/A/A/TRAAAAK128F9318786.json')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   song_id           1 non-null      object \n",
      " 1   num_songs         1 non-null      int64  \n",
      " 2   title             1 non-null      object \n",
      " 3   artist_name       1 non-null      object \n",
      " 4   artist_latitude   0 non-null      float64\n",
      " 5   year              1 non-null      int64  \n",
      " 6   duration          1 non-null      float64\n",
      " 7   artist_id         1 non-null      object \n",
      " 8   artist_longitude  0 non-null      float64\n",
      " 9   artist_location   1 non-null      object \n",
      "dtypes: float64(3), int64(2), object(5)\n",
      "memory usage: 208.0+ bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "song_data_bucket = s3.Bucket(BUCKET)\n",
    "\n",
    "first_object = None\n",
    "\n",
    "for my_object in song_data_bucket.objects.filter(Prefix='song-data').limit(2):\n",
    "    if my_object.key.endswith('.json'):\n",
    "        first_object = my_object\n",
    "        print(my_object)\n",
    "\n",
    "s3_client = boto3.client('s3', region_name='us-west-2', aws_access_key_id=KEY, aws_secret_access_key=SECRET)\n",
    "json_obj = s3_client.get_object(Bucket=BUCKET, Key=first_object.key)\n",
    "json_data = json_obj[\"Body\"].read().decode('utf-8')\n",
    "json_df = pd.read_json(json_data, lines=True)\n",
    "print(json_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 Creating a new IAM Role\n",
      "An error occurred (EntityAlreadyExists) when calling the CreateRole operation: Role with name dwhRole already exists.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create the IAM role\n",
    "try:\n",
    "    print('1.1 Creating a new IAM Role')\n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=DWH_IAM_ROLE_NAME,\n",
    "        Description=\"Allows Redshift clusters to call AWS services on your behalf\",\n",
    "        AssumeRolePolicyDocument= json.dumps(\n",
    "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "                            'Effect': 'Allow',\n",
    "                            'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "            'Version': '2012-10-17'}\n",
    "        )\n",
    "    )\n",
    "    iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
    "                        PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                        )['ResponseMetadata']['HTTPStatusCode']\n",
    "    \n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get the IAM role ARN\n",
      "arn:aws:iam::191954599309:role/dwhRole\n"
     ]
    }
   ],
   "source": [
    "# Get ARN role\n",
    "print(\"Get the IAM role ARN\")\n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "print(roleArn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RedShift cluster\n",
    "try:\n",
    "    response = redshift.create_cluster(        \n",
    "        # TODO: add parameters for hardware\n",
    "        ClusterType = DWH_CLUSTER_TYPE,\n",
    "        NodeType = DWH_NODE_TYPE,\n",
    "        NumberOfNodes = int(DWH_NUM_NODES),\n",
    "\n",
    "        # TODO: add parameters for identifiers & credentials\n",
    "        DBName = DB_NAME,\n",
    "        ClusterIdentifier = DWH_CLUSTER_IDENTIFIER,\n",
    "        MasterUsername = DB_USER,\n",
    "        MasterUserPassword = DB_PASSWORD,\n",
    "        \n",
    "        # TODO: add parameter for role (to allow s3 access)\n",
    "        IamRoles = [roleArn]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ClusterIdentifier</td>\n",
       "      <td>dwhcluster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NodeType</td>\n",
       "      <td>dc2.large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ClusterStatus</td>\n",
       "      <td>available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MasterUsername</td>\n",
       "      <td>dwhuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DBName</td>\n",
       "      <td>dwh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Endpoint</td>\n",
       "      <td>{'Address': 'dwhcluster.cypufzdprkhj.us-west-2.redshift.amazonaws.com', 'Port': 5439}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>VpcId</td>\n",
       "      <td>vpc-09c9d46d904b51d58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NumberOfNodes</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Key  \\\n",
       "0  ClusterIdentifier   \n",
       "1           NodeType   \n",
       "2      ClusterStatus   \n",
       "3     MasterUsername   \n",
       "4             DBName   \n",
       "5           Endpoint   \n",
       "6              VpcId   \n",
       "7      NumberOfNodes   \n",
       "\n",
       "                                                                                   Value  \n",
       "0                                                                             dwhcluster  \n",
       "1                                                                              dc2.large  \n",
       "2                                                                              available  \n",
       "3                                                                                dwhuser  \n",
       "4                                                                                    dwh  \n",
       "5  {'Address': 'dwhcluster.cypufzdprkhj.us-west-2.redshift.amazonaws.com', 'Port': 5439}  \n",
       "6                                                                  vpc-09c9d46d904b51d58  \n",
       "7                                                                                      4  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DWH_ENDPOINT ::  dwhcluster.cypufzdprkhj.us-west-2.redshift.amazonaws.com\n",
      "DWH_ROLE_ARN ::  arn:aws:iam::191954599309:role/dwhRole\n"
     ]
    }
   ],
   "source": [
    "DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "print(\"DWH_ENDPOINT :: \", DWH_ENDPOINT)\n",
    "print(\"DWH_ROLE_ARN :: \", roleArn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ec2.SecurityGroup(id='sg-057e19db67039a2dd')\n",
      "An error occurred (InvalidPermission.Duplicate) when calling the AuthorizeSecurityGroupIngress operation: the specified rule \"peer: 0.0.0.0/0, TCP, from port: 5439, to port: 5439, ALLOW\" already exists\n"
     ]
    }
   ],
   "source": [
    "# Open incoming TCP port to access cluster endpoint\n",
    "try:\n",
    "    vpc = ec2.Vpc(id=myClusterProps['VpcId'])\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    print(defaultSg)\n",
    "    \n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName= defaultSg.group_name,  # TODO: fill out\n",
    "        CidrIp='0.0.0.0/0',  # TODO: fill out\n",
    "        IpProtocol='TCP',  # TODO: fill out\n",
    "        FromPort=int(DB_PORT),\n",
    "        ToPort=int(DB_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://dwhuser:OkeMog2014@dwhcluster.cypufzdprkhj.us-west-2.redshift.amazonaws.com:5439/dwh\n"
     ]
    }
   ],
   "source": [
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DB_USER, DB_PASSWORD, DWH_ENDPOINT, DB_PORT,DB_NAME)\n",
    "print(conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cypufzdprkhj.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "(psycopg2.errors.UndefinedTable) relation \"staging_events\" does not exist\n",
      "\n",
      "[SQL: SELECT COUNT(*) FROM staging_events;]\n",
      "(Background on this error at: https://sqlalche.me/e/14/f405)\n"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT COUNT(*)\n",
    "FROM staging_events;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
